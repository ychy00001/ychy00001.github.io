<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Git添加多个ssh-key</title>
    <url>/2018/11/Git%E6%B7%BB%E5%8A%A0%E5%A4%9A%E4%B8%AAssh-key/</url>
    <content><![CDATA[<p>工作中经常本机有自己github仓库的ssh，公司又需要添加公司内部的gitlab的ssh，这样就需要配置多个ssh-key。本文简单描述了如何创建多个ssh-key并配置使用。 </p>
<span id="more"></span>
<h1 id="生成ssh-key"><a href="#生成ssh-key" class="headerlink" title="生成ssh-key"></a>生成ssh-key</h1><h2 id="添加ssh-key不要三次回车"><a href="#添加ssh-key不要三次回车" class="headerlink" title="添加ssh-key不要三次回车"></a>添加ssh-key不要三次回车</h2><pre><code>ssh-keygen -t rsa -C &quot;example@mail.com&quot;
</code></pre>
<h2 id="输入完上述命令后"><a href="#输入完上述命令后" class="headerlink" title="输入完上述命令后"></a>输入完上述命令后</h2><p><img src="/2018/11/Git%E6%B7%BB%E5%8A%A0%E5%A4%9A%E4%B8%AAssh-key/1.png"><br>在这里输入对应要创建的ssh_key的名称，默认为id_rsa ，可以填写id_rsa_example再按两次回车。</p>
<h1 id="添加配置文件"><a href="#添加配置文件" class="headerlink" title="添加配置文件"></a>添加配置文件</h1><pre><code>vim ~/.ssh/config
</code></pre>
<h2 id="配置ssh对应的key"><a href="#配置ssh对应的key" class="headerlink" title="配置ssh对应的key"></a>配置ssh对应的key</h2><pre><code>Host github.com
    HostName github.com
    User git
    IdentityFile ~/.ssh/id_rsa

Host git.example.com
    HostName git.example.com    //这个就是公司的host
    User git
    IdentityFile ~/.ssh/id_rsa_example
</code></pre>
<h2 id="添加私钥"><a href="#添加私钥" class="headerlink" title="添加私钥"></a>添加私钥</h2><pre><code>ssh-add ~/.ssh/id_rsa_example
</code></pre>
<h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><pre><code>ssh -vT git@github.com
</code></pre>
<h1 id="添加至github-gitlab"><a href="#添加至github-gitlab" class="headerlink" title="添加至github/gitlab"></a>添加至github/gitlab</h1><p>最后将~/.ssh/xxxx.pub放至对应的github/gitlab的ssh配置中即可。（公钥需要与config文件中HostName配置的私钥对应）</p>
]]></content>
      <categories>
        <category>研发工具</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title>shell-if[command]参数整理</title>
    <url>/2016/07/shell-if-command-%E5%8F%82%E6%95%B0%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<p>[-a file] 如果file存在则为真<br>[-b file] 如果file存在且是一个块特殊文件则为真<br>[-c file] 如果file存在且是一个字特殊文件则为真 </p>
<span id="more"></span>
<p>[-d file] 如果file文件存在且是一个目录则为真<br>[-e file] 如果file文件存在则为真<br>[-f file] 如果file存在且是一个普通文件则为真<br>[-g file] 如果file存在且已经设置了SGID则为真（SUID 是 Set User ID, SGID 是 Set Group ID的意思）<br>[-h file] 如果file存在且是一个符号连接则为真<br>[-k file] 如果file存在且已经设置sticky则为真 ，（sticky标识该文件/目录是任何人都可以写,但也只有文件的属主才可以删除文件，linux下/tmp目录就是这样的目录）<br>[-p file] 如果file存在且是一个名字管道（F如果O）则为真 （管道是linux里面进程间通信的一种方式，其他的还有像信号（signal）、信号量、消息队列、共享内存、套接字（socket）等）<br>[-r file] 如果file存在且是可读的则为真<br>[-s file] 如果file存在且大小不为0则为真<br>[-t FD] 如果文件描述符FD打开且指向一个终端则为真<br>[-u file] 如果file存在且设置了SUID（set userID）则为真<br>[-w file] 如果file存在且是可写的则为真<br>[-x file] 如果file存在且是可执行的则为真<br>[-O file] 如果file存在且属有效用户ID则为真<br>[-G file] 如果file存在且属有效用户组则为真<br>[-L file] 如果file存在且是一个符号连接则为真<br>[-N file] 如果file存在并且自上次读取后被修改则为真<br>[-S file] 如果file存在且是一个套接字则为真<br>[file1 –nt file2] 如果file1更新时间大于file2或者file1 存在 file2 不存在则为真<br>[file1 –ot file2] 如果file1比file2要老，或者file2存在且file1不存在则为真<br>[file1 –ef file2] 如果file1和file2指向相同的设备和节点号则为真<br>[-o optionname] 如果shell选项“optionname”开启则为真<br>[-z string] “string”的长度为零则为真<br>[-n string] or [string] “string”的长度为非零non-zero则为真<br>[sting 1== string2] 如果2个字符串相同。“=”may be used instead of “==”for strict posix compliance则为真<br>[string1 != string2] 如果字符串不相等则为真<br>[string1 &lt; string2] string1的字典顺序在string2之前则为真<br>[arg1 OP arg2] “OP”可以是以下几个操作（-eq  : 等于、-ne : 不等于、-lt : 小于、-le : 小于等于、-gt : 大于、–ge : 大于等于）</p>
<blockquote>
<p>参考文章：<a href="https://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html">https://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html</a></p>
</blockquote>
]]></content>
      <categories>
        <category>Linux</category>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>认识MyBatis</title>
    <url>/2021/06/%E8%AE%A4%E8%AF%86MyBatis/</url>
    <content><![CDATA[<p>MyBatis是java开发中常用的ORM框架，本文介绍MyBatis的入门使用。</p>
<h1 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h1><p>官网的简介写的比较好，建议大家直接去官网查看。<br><a href="https://mybatis.org/mybatis-3/zh/index.html">https://mybatis.org/mybatis-3/zh/index.html</a><br>简单点说MyBatis就是方便代码中操作数据库，避免了JDBS繁琐的设置及比编写。</p>
<span id="more"></span>
<blockquote>
<p>其他ORM框架</p>
</blockquote>
<ul>
<li>JDBC</li>
<li>JdbcTemplate</li>
<li>Hibernate </li>
</ul>
<h1 id="二、简单使用"><a href="#二、简单使用" class="headerlink" title="二、简单使用"></a>二、简单使用</h1><h4 id="1-创建maven项目-使用IDEA可以直接创建"><a href="#1-创建maven项目-使用IDEA可以直接创建" class="headerlink" title="1. 创建maven项目 使用IDEA可以直接创建"></a>1. 创建maven项目 使用IDEA可以直接创建</h4><h4 id="2-在resource目录创建mybatis-config-xml文件"><a href="#2-在resource目录创建mybatis-config-xml文件" class="headerlink" title="2. 在resource目录创建mybatis-config.xml文件"></a>2. 在resource目录创建mybatis-config.xml文件</h4><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
&lt;!DOCTYPE configuration
        PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;
        &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;
&lt;configuration&gt;
    &lt;environments default=&quot;development&quot;&gt;
        &lt;environment id=&quot;development&quot;&gt;
            &lt;transactionManager type=&quot;JDBC&quot;/&gt;
            &lt;dataSource type=&quot;POOLED&quot;&gt;
                &lt;property name=&quot;driver&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot;/&gt;
                &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://$&#123;host&#125;/$&#123;database&#125;&quot;/&gt;
                &lt;property name=&quot;username&quot; value=&quot;$&#123;username&#125;&quot;/&gt;
                &lt;property name=&quot;password&quot; value=&quot;$&#123;password&#125;&quot;/&gt;
            &lt;/dataSource&gt;
        &lt;/environment&gt;
    &lt;/environments&gt;
    &lt;mappers&gt;
        &lt;mapper resource=&quot;mapper/UserMapper.xml&quot;/&gt;
    &lt;/mappers&gt;
&lt;/configuration&gt;
</code></pre>
<blockquote>
<p>注意 代码中${host}/${database} ${username}  ${password} 需要替换成自己对应的真实账号密码及链接路径</p>
</blockquote>
<h4 id="3-在resource目录创建mapper目录并创建UserMapper-xml文件"><a href="#3-在resource目录创建mapper目录并创建UserMapper-xml文件" class="headerlink" title="3. 在resource目录创建mapper目录并创建UserMapper.xml文件"></a>3. 在resource目录创建mapper目录并创建UserMapper.xml文件</h4><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
&lt;!DOCTYPE mapper
        PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;
        &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;
&lt;mapper namespace=&quot;com.ychy.mybatis.mapper.UserMapper&quot;&gt;
    &lt;select id=&quot;selectUser&quot; resultType=&quot;com.ychy.mybatis.dao.User&quot;&gt;
        select * from User where id = #&#123;id&#125;
    &lt;/select&gt;
&lt;/mapper&gt;
</code></pre>
<blockquote>
<p>注意：namespace不是必须有这个包，mybatis使用namespace使用更长的限定名来将sql语句隔离开。<br>但是namespace我们都遵循这个规范就好，按照包名加类名来写，这样IDEA有插件可以方便跳转映射关系。见后面：使用正确描述每个语句的参数和返回值的接口UserMapper.java</p>
</blockquote>
<h4 id="4-单元测试"><a href="#4-单元测试" class="headerlink" title="4. 单元测试"></a>4. 单元测试</h4><pre><code class="java">public class MyBatisTest &#123;
    @Test
    public void testUserSelectOne() throws Exception&#123;
        String resource = &quot;mybatis-config.xml&quot;;
        InputStream inputStream = Resources.getResourceAsStream(resource);
        SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream);
        SqlSession sqlSession = sqlSessionFactory.openSession();
        // 第一种方式 直接调用
        User user = sqlSession.selectOne(&quot;com.ychy.mybatis.mapper.UserMapper.selectUser&quot;,1);
        System.out.println(user);
    &#125;
&#125;
</code></pre>
<blockquote>
<p>这里sqlSession调用selectOne方法中第一个参数就是UserMapper.xml中namespace+id凭借而成的。<br>第二个参数为要查询的ID</p>
</blockquote>
<h4 id="5-项目依赖"><a href="#5-项目依赖" class="headerlink" title="5.项目依赖"></a>5.项目依赖</h4><pre><code class="xml">  &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.mybatis&lt;/groupId&gt;
            &lt;artifactId&gt;mybatis&lt;/artifactId&gt;
            &lt;version&gt;3.5.3&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;mysql&lt;/groupId&gt;
            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
            &lt;version&gt;8.0.19&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
            &lt;version&gt;1.18.12&lt;/version&gt;
            &lt;scope&gt;provided&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt;
            &lt;artifactId&gt;junit-jupiter&lt;/artifactId&gt;
            &lt;version&gt;$&#123;junit.jupiter.version&#125;&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.junit.platform&lt;/groupId&gt;
            &lt;artifactId&gt;junit-platform-launcher&lt;/artifactId&gt;
            &lt;version&gt;1.6.0&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;

    &lt;/dependencies&gt;
</code></pre>
<h4 id="6-接口式编程（使用正确描述每个语句的参数和返回值的接口-gt-UserMapper-java）"><a href="#6-接口式编程（使用正确描述每个语句的参数和返回值的接口-gt-UserMapper-java）" class="headerlink" title="6. 接口式编程（使用正确描述每个语句的参数和返回值的接口-&gt;UserMapper.java）"></a>6. 接口式编程（使用正确描述每个语句的参数和返回值的接口-&gt;UserMapper.java）</h4><pre><code class="java">package com.ychy.mybatis.mapper;

import com.ychy.mybatis.dao.User;

public interface UserMapper &#123;
     User selectUser(int id);
&#125;
</code></pre>
<blockquote>
<p>为什么可以直接通过UserMapper接口就可以查询User呢？因为Mybatis底层使用了动态代理类。</p>
</blockquote>
<h5 id="单元测试可以更改如下"><a href="#单元测试可以更改如下" class="headerlink" title="单元测试可以更改如下"></a>单元测试可以更改如下</h5><pre><code class="java">public class MyBatisTest &#123;
    @Test
    public void testUserSelectOne() throws Exception&#123;
        String resource = &quot;mybatis-config.xml&quot;;
        InputStream inputStream = Resources.getResourceAsStream(resource);
        SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream);
        SqlSession sqlSession = sqlSessionFactory.openSession();
        // 第一种方式 直接调用
// User user = sqlSession.selectOne(&quot;com.ychy.mybatis.mapper.UserMapper.selectUser&quot;,1);
        // 第二种方式获取UserMapper 第二种方法有很多优势，首先它不依赖于字符串字面值，会更安全一点； 其次，如果你的 IDE 有代码补全功能，那么代码补全可以帮你快速选择已映射的 SQL 语句。
        UserMapper userMapper = sqlSession.getMapper(UserMapper.class);
        User user = userMapper.selectUser(1);
        System.out.println(user);
    &#125;
&#125;
</code></pre>
<blockquote>
<p>这样不仅可以执行更清晰和类型安全的代码，而且还不用担心易错的字符串字面值以及强制类型转换</p>
</blockquote>
<h4 id="7-使用注解的方式访问"><a href="#7-使用注解的方式访问" class="headerlink" title="7. 使用注解的方式访问"></a>7. 使用注解的方式访问</h4><pre><code class="java">public interface UserMapper &#123;
     @Select(&quot;SELECT * FROM user WHERE id = #&#123;id&#125;&quot;)
     User selectUser(int id);
&#125;
</code></pre>
<h5 id="同时需要更改mybatis-config-xml"><a href="#同时需要更改mybatis-config-xml" class="headerlink" title="同时需要更改mybatis-config.xml"></a>同时需要更改mybatis-config.xml</h5><pre><code class="xml">    &lt;mappers&gt;
        &lt;!-- 使用xml方式 --&gt;
&lt;!-- &lt;mapper resource=&quot;mapper/UserMapper.xml&quot;/&gt; --&gt;
        &lt;!-- 使用注解方式方式 --&gt;
        &lt;mapper class=&quot;com.ychy.mybatis.mapper.UserMapper&quot;&gt;&lt;/mapper&gt;
    &lt;/mappers&gt;
</code></pre>
<blockquote>
<p>这样可以不用写UserMapper.xml文件，但是需要更改mybatis-config.xml中mappers的内容</p>
</blockquote>
<h4 id="8-注意：注解和xml形式不可同时使用，否则会报如下错误"><a href="#8-注意：注解和xml形式不可同时使用，否则会报如下错误" class="headerlink" title="8.注意：注解和xml形式不可同时使用，否则会报如下错误"></a>8.注意：注解和xml形式不可同时使用，否则会报如下错误</h4><pre><code>org.apache.ibatis.exceptions.PersistenceException: 
### Error building SqlSession.
### The error may exist in com/ychy/mybatis/mapper/UserMapper.java (best guess)
### Cause: org.apache.ibatis.builder.BuilderException: Error parsing SQL Mapper Configuration. Cause: java.lang.IllegalArgumentException: Mapped Statements collection already contains value for com.ychy.mybatis.mapper.UserMapper.selectUser. please check mapper/UserMapper.xml and com/ychy/mybatis/mapper/UserMapper.java (best guess)
</code></pre>
]]></content>
      <categories>
        <category>ORM框架</category>
        <category>MyBatis</category>
      </categories>
      <tags>
        <tag>MyBatis</tag>
        <tag>ORM</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux exec命令使用</title>
    <url>/2021/06/Linux-exec%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>本文介绍exec的基本功能及使用方式。</p>
<h2 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h2><p>看到ngxin的docker-entrypoint.sh中写有exec相关命令不清楚为何如此使用，如下：</p>
<pre><code>#!/bin/sh
set -e
if [ -z &quot;$&#123;NGINX_ENTRYPOINT_QUIET_LOGS:-&#125;&quot; ]; then
    exec 3&gt;&amp;1
else
    exec 3&gt;/dev/null
fi
.....
</code></pre>
<span id="more"></span>
<p>在CentOS Linux release 7.9下查看man手册</p>
<pre><code>man exec
</code></pre>
<p>如何所示，该命令是bash中内嵌的命令：<br><img src="/2021/06/Linux-exec%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/1.png"></p>
<p>我们找到exec的描述信息：<br><img src="/2021/06/Linux-exec%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/2.png"></p>
<p>如果指定该命令，他会替换当前shell。并且不会创建新的进程。</p>
<h2 id="fork简介"><a href="#fork简介" class="headerlink" title="fork简介"></a>fork简介</h2><p>上面提到不会现在新的进程，这里不得不提一下linux的子进程概念。</p>
<p>fork命令是linux的系统调用，用来创建子进程。</p>
<blockquote>
<p>子进程概念：由另外一个进程（对应称之为父进程）所创建的进程。 子进程继承了父进程的大部分属性，例如文件描述符。</p>
</blockquote>
<p>我们在写shell时候会在开头定义：#!/bin/sh。这样在执行该xxx.sh文件时，系统会使用fork命令，重新开一个sub-shell来执行你的shell脚本。</p>
<h2 id="exec的执行"><a href="#exec的执行" class="headerlink" title="exec的执行"></a>exec的执行</h2><p>回到我们的exec命令，执行exec命令时，系统不会fork子进程，而是会用一个新的进程镜像来替换当前进程镜像。新的进程跟fork后的进程不一样，新的进程跟旧进程是有同样的pid（进程id）和同样的上下文环境。它将程序加载到当前进程空间，并在入口点运行exec需要执行的程序。</p>
<h2 id="总结：exec和fork的异同"><a href="#总结：exec和fork的异同" class="headerlink" title="总结：exec和fork的异同"></a>总结：exec和fork的异同</h2><p>fork: 创建新的子进程，父进程和子进程同时运行<br>exec: 创建进程镜像替换旧进程，没有父子进程一说。</p>
<h2 id="exec的应用"><a href="#exec的应用" class="headerlink" title="exec的应用"></a>exec的应用</h2><h3 id="exec-执行某个指令"><a href="#exec-执行某个指令" class="headerlink" title="exec 执行某个指令"></a>exec 执行某个指令</h3><p>exec执行某个命令会执行完并退出bash<br><img src="/2021/06/Linux-exec%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/3.png"></p>
<h3 id="exec-不执行命令"><a href="#exec-不执行命令" class="headerlink" title="exec 不执行命令"></a>exec 不执行命令</h3><p>exec未携带命令执行表示重定向bash中内容<br><img src="/2021/06/Linux-exec%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/4.png"></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>回到我们开头解释我们nginx的docker-entrypoint.sh，如下代码：</p>
<pre><code># 如果没有设置（-z 如果字符串为空则返回true）NGINX_ENTRYPOINT_QUIET_LOGS标识
if [ -z &quot;$&#123;NGINX_ENTRYPOINT_QUIET_LOGS:-&#125;&quot; ]; then
    # 将标准错误输出（3：stderr）重定向至标准输出中（1：stdout，&amp; ：引用标识）
    exec 3&gt;&amp;1
else
    # 如果存在NGINX_ENTRYPOINT_QUIET_LOGS则将标准错误输出内容移除
    exec 3&gt;/dev/null
fi
</code></pre>
<blockquote>
<p>上述代码中3、1是shell文件描述符相关支持，参考后续博文介绍</p>
</blockquote>
<p>参考文献：<br><a href="https://www.geeksforgeeks.org/difference-fork-exec/">https://www.geeksforgeeks.org/difference-fork-exec/</a><br><a href="http://xstarcd.github.io/wiki/shell/exec_redirect.html">http://xstarcd.github.io/wiki/shell/exec_redirect.html</a></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>RabbitMQ-Federation Plugin使用示例</title>
    <url>/2021/06/RabbitMQ-Federation-Plugin%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/</url>
    <content><![CDATA[<p>本文采用docker环境来做Federation Plugin配置的流程。</p>
<h1 id="基础环境准备"><a href="#基础环境准备" class="headerlink" title="基础环境准备"></a>基础环境准备</h1><h2 id="基础网络"><a href="#基础网络" class="headerlink" title="基础网络"></a>基础网络</h2><table>
<thead>
<tr>
<th>名称</th>
<th>ip</th>
<th>操作系统</th>
<th>mq版本</th>
</tr>
</thead>
<tbody><tr>
<td>rabbitmq</td>
<td>172.21.0.2</td>
<td>centos 8</td>
<td>3.x</td>
</tr>
<tr>
<td>rabbitmq-upstream</td>
<td>172.21.0.3</td>
<td>centos 8</td>
<td>3.x</td>
</tr>
</tbody></table>
<span id="more"></span>
<h2 id="自用docker-compose镜像"><a href="#自用docker-compose镜像" class="headerlink" title="自用docker-compose镜像"></a>自用docker-compose镜像</h2><pre><code>version: &#39;3&#39;

services:
  rabbitmq:
    # 管理界面 guest/guest
    image: rabbitmq:3-management
    container_name: rabbitmq
    hostname: my-rabbit
    volumes:
      - ./rabbitmq/conf/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf
      - ./rabbitmq/data:/var/lib/rabbitmq/mnesia
    ports:
      - &#39;5672:5672&#39;
      - &#39;15672:15672&#39;
    networks:
      - rabbitmq-network

  rabbitmq-upstream:
    image: rabbitmq:3-management
    container_name: rabbitmq-upstream
    hostname: my-rabbit-upstream
    volumes:
      - ./rabbitmq/conf-upstream/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf
      - ./rabbitmq/data-upstream:/var/lib/rabbitmq/mnesia
    ports:
      - &#39;5673:5672&#39;
      - &#39;15673:15672&#39;
    networks:
      - rabbitmq-network

networks:
  rabbitmq-network:
</code></pre>
<h2 id="运行docker-compose"><a href="#运行docker-compose" class="headerlink" title="运行docker-compose"></a>运行docker-compose</h2><pre><code>docker-compose up -d
</code></pre>
<h2 id="进入两个容器分别执行加载federation插件"><a href="#进入两个容器分别执行加载federation插件" class="headerlink" title="进入两个容器分别执行加载federation插件"></a>进入两个容器分别执行加载federation插件</h2><pre><code># 进入rabbitmq
docker-compose exec rabbitmq bash
rabbitmq-plugins enable rabbitmq_federation
rabbitmq-plugins enable rabbitmq_federation_management
# 进入rabbitmq-upstream
docker-compose exec rabbitmq-upstream bash
rabbitmq-plugins enable rabbitmq_federation
rabbitmq-plugins enable rabbitmq_federation_management
</code></pre>
<h2 id="查看容器ip"><a href="#查看容器ip" class="headerlink" title="查看容器ip"></a>查看容器ip</h2><pre><code>ifconfig
</code></pre>
<p>当前我的两个容器ip如下<br>|名称|ip|<br>|—|—|<br>|rabbitmq|172.21.0.3|<br>|rabbitmq-upstream|172.21.0.2|</p>
<h2 id="在rabbitmq容器设置upstream"><a href="#在rabbitmq容器设置upstream" class="headerlink" title="在rabbitmq容器设置upstream"></a>在rabbitmq容器设置upstream</h2><pre><code># 已在容器中不需要执行该命令
docker-compose exec rabbitmq bash
rabbitmqctl set_parameter federation-upstream my-upstream \
&#39;&#123;&quot;uri&quot;:&quot;amqp://guest:guest@172.21.0.2:5672&quot;,&quot;expires&quot;:3600000&#125;&#39;
</code></pre>
<p>设置好后ui界面会显示出对应的upstream。<br><img src="/2021/06/RabbitMQ-Federation-Plugin%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/1.png"></p>
<h2 id="在rabbitmq容器设置policy"><a href="#在rabbitmq容器设置policy" class="headerlink" title="在rabbitmq容器设置policy"></a>在rabbitmq容器设置policy</h2><pre><code># 已在容器中不需要执行该命令
docker-compose exec rabbitmq bash
rabbitmqctl set_policy --apply-to exchanges federate-me &quot;^amq\.&quot; &#39;&#123;&quot;federation-upstream-set&quot;:&quot;all&quot;&#125;&#39;
</code></pre>
<p>执行后可以查看federation对应UI界面会将本地的exchange与federate-me policy绑定<br><img src="/2021/06/RabbitMQ-Federation-Plugin%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/2.png"></p>
<p>同时federation-upstream容器对应UI界面会有对应的绑定关系展示<br><img src="/2021/06/RabbitMQ-Federation-Plugin%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/3.png"></p>
<h2 id="代码测试（python）"><a href="#代码测试（python）" class="headerlink" title="代码测试（python）"></a>代码测试（python）</h2><h3 id="生产者代码"><a href="#生产者代码" class="headerlink" title="生产者代码"></a>生产者代码</h3><p>生产者代码链接rabbitmq-upstream用于发送消息。</p>
<pre><code>import pika
import sys
import os

def main():
    # 创建链接
    connection = pika.BlockingConnection(pika.ConnectionParameters(&#39;localhost&#39;, &#39;5673&#39;))
    channel = connection.channel()
    # 声明一个本地upstream消息的队列
    channel.queue_declare(queue=&#39;receive-upstream&#39;)
    # 发送消息至exchange
    channel.basic_publish(exchange=&#39;amq.direct&#39;, routing_key=&#39;receive-upstream&#39;, body=&#39;Hello World!&#39;)
    print(&quot;[x] Sent &#39;Hello World!&#39;&quot;)
    connection.close()

if __name__ == &#39;__main__&#39;:
    try:
        main()
    except KeyboardInterrupt:
        print(&#39;Interrupted&#39;)
        try:
            sys.exit(0)
        except SystemExit:
            os._exit(0)
</code></pre>
<h3 id="消费者代码"><a href="#消费者代码" class="headerlink" title="消费者代码"></a>消费者代码</h3><p>生产者代码链接rabbitmq用于接收upstream的消息。</p>
<pre><code>import pika
import os
import sys
import time

def main():
    # 创建链接
    connection = pika.BlockingConnection(pika.ConnectionParameters(host=&#39;localhost&#39;, port=&#39;5672&#39;))
    channel = connection.channel()
    # 声明一个接受upstream消息的队列
    channel.queue_declare(queue=&#39;receive-upstream&#39;)
    # 绑定队列至接受upstream-exchange的一个exchange上
    channel.queue_bind(&#39;receive-upstream&#39;, &#39;amq.direct&#39;)

    # 接收消息，定义callback函数，接收消息回调该函数
    def callback(ch, method, properties, body):
        print(&quot;[x] Received %r &quot; % body)
        ch.basic_ack(delivery_tag=method.delivery_tag)

    channel.basic_consume(queue=&#39;receive-upstream&#39;, auto_ack=False, on_message_callback=callback)
    print(&#39; [*] Waiting for messages. To exit press CTRL+C&#39;)
    channel.start_consuming()

if __name__ == &#39;__main__&#39;:
    try:
        main()
    except KeyboardInterrupt:
        print(&#39;Interrupted&#39;)
        try:
            sys.exit(0)
        except SystemExit:
            os._exit(0)
</code></pre>
<h3 id="运行截图"><a href="#运行截图" class="headerlink" title="运行截图"></a>运行截图</h3><p><img src="/2021/06/RabbitMQ-Federation-Plugin%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/4.png"><br><img src="/2021/06/RabbitMQ-Federation-Plugin%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/5.png"></p>
]]></content>
      <categories>
        <category>消息中间件</category>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
        <tag>MQ</tag>
        <tag>Federation</tag>
      </tags>
  </entry>
  <entry>
    <title>RabbitMQ-Federation Plugin概述</title>
    <url>/2021/06/RabbitMQ-Federation-Plugin%E6%A6%82%E8%BF%B0/</url>
    <content><![CDATA[<p>最进在研究联邦学习框架FATE，FATE的部署方式有多种，其中有一种部署方式是基于spark来做分布式计算，并且使用Rabbitmq跨节点数据传输的（Federation Plug）。<br>文件简单介绍Rabbitmq的插件Federation Plugin的基本使用流程。</p>
<span id="more"></span>
<h1 id="简述FederationPlugin"><a href="#简述FederationPlugin" class="headerlink" title="简述FederationPlugin"></a>简述FederationPlugin</h1><p>FederationPlugin可以简单理解为不需要建立集群就可以在多个MQ服务的做消息传输。这样不建立集群就可以在不同的网络域下作数据传输。<br>如下图，可以基于队列消息传输，也可以基于exchange传输。<br><img src="/2021/06/RabbitMQ-Federation-Plugin%E6%A6%82%E8%BF%B0/1.png"></p>
<h1 id="基础使用流程"><a href="#基础使用流程" class="headerlink" title="基础使用流程"></a>基础使用流程</h1><h3 id="1-开启Federation插件"><a href="#1-开启Federation插件" class="headerlink" title="1. 开启Federation插件"></a>1. 开启Federation插件</h3><p>启用rabbitmq_federation插件，可选启用rabbitmq_federation_management插件，用于控制台界面操作。</p>
<pre><code>rabbitmq-plugins enable rabbitmq_federation
rabbitmq-plugins enable rabbitmq_federation_management
</code></pre>
<h3 id="2-设置upstream"><a href="#2-设置upstream" class="headerlink" title="2. 设置upstream"></a>2. 设置upstream</h3><p>upstream就是一个上游服务，假设现在有节点A和节点B两方，B想要消费A中的队列，则A就是B的upstream。<br><img src="/2021/06/RabbitMQ-Federation-Plugin%E6%A6%82%E8%BF%B0/2.png"></p>
<p>下述server-name就是上图172.xx.xx.xx所在的broker。<br>server-name格式具体请产品AMQP-URL：<a href="https://www.rabbitmq.com/uri-spec.html">https://www.rabbitmq.com/uri-spec.html</a><br>简单举例：<br>|URI|Username|Passowd|Host|Port|Vhost|<br>|—|—|—|—|—|—|<br>|amqp://user:pass@host:10000/vhost|”user”|”pass”|”host”|10000|”vhost”|<br>|amqp://user%61:%61pass@ho%61st:10000/v%2fhost|”usera”|”apass”|”hoast”|10000|”v/host”|<br>|amqp://host|||”host”|||</p>
<p>下述代码定义了一个名为my-upstream的federation-upstream，指定uri和expires，expires为消息的过期时间单位ms。</p>
<pre><code>rabbitmqctl set_parameter federation-upstream my-upstream \
&#39;&#123;&quot;uri&quot;:&quot;amqp://server-name&quot;,&quot;expires&quot;:3600000&#125;&#39;
</code></pre>
<blockquote>
<p>本文只展示了命令行设置方式，额外方式如：http、web UI等方式请参考官方文档<br><a href="https://www.rabbitmq.com/federation.html">https://www.rabbitmq.com/federation.html</a></p>
</blockquote>
<h3 id="3-设置policy"><a href="#3-设置policy" class="headerlink" title="3. 设置policy"></a>3. 设置policy</h3><p>policy可以理解为一个运行时配置，可以再mq运行时设置更改相关配置信息，具体参考官方文档：<a href="https://www.rabbitmq.com/parameters.html%E3%80%82">https://www.rabbitmq.com/parameters.html。</a><br>下述设置了一个policy，federate-me为自定义的名称，”^amq.“是正则表达式，用来匹配对应的exchanges，federation-upstream-set:”all”表示匹配所有的upstream。<br>概括下来就是，匹配所有amq.开头的exchange用来接收upstream消息</p>
<pre><code>rabbitmqctl set_policy --apply-to exchanges federate-me &quot;^amq\.&quot; &#39;&#123;&quot;federation-upstream-set&quot;:&quot;all&quot;&#125;&#39;
</code></pre>
<h3 id="4-写代码测试"><a href="#4-写代码测试" class="headerlink" title="4. 写代码测试"></a>4. 写代码测试</h3><p>上述正常配置后，假设有AB两个节点，B节点设置upstream为A，则A的所有消息都将被B接收。</p>
<blockquote>
<p>具体示例参考后续文章【RabbitMQ-Federation Plugin使用示例】</p>
</blockquote>
]]></content>
      <categories>
        <category>消息中间件</category>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
        <tag>MQ</tag>
        <tag>Federation</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark on YARN最大化执行任务配置</title>
    <url>/2021/07/Spark-on-YARN%E6%9C%80%E5%A4%A7%E5%8C%96%E6%89%A7%E8%A1%8C%E4%BB%BB%E5%8A%A1%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h1 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h1><p>本文简述spark调优历程，java服务端研发一枚，被要求搭建hadoop-spark环境运行机器学习。搭建完成后基本使用默认的配置文件执行任务，测试使用500MB的数据来运行逻辑回归，执行时间非常长，为了缩短任务运行时间，开始了无止尽的摸索参数配置。</p>
<span id="more"></span>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><ul>
<li>本文采用三台4H8G机器为测试机器，配置只是为了让单个任务最大化使用系统资源。</li>
<li>本文配置文件内容仅供参考，需要根据实际机器资源和任务使用情况来自行设置</li>
<li>本文描述系统的CPU和内存资源时简写，示例：4H8G表示4个cpu核数，8GB的内存</li>
</ul>
<h1 id="yarn基础配置"><a href="#yarn基础配置" class="headerlink" title="yarn基础配置"></a>yarn基础配置</h1><p>spark是运行在yarn中的，所以先从yarn的配置文件下手，单台服务器为4H8G，预留1H1G给操作系统，剩下的全部配置给yarn使用。因为当前只需要一个任务最大化使用系统资源，所以最大可分配内存可以跟节点总内存一致。</p>
<h2 id="内存相关（yarn-site-xml）"><a href="#内存相关（yarn-site-xml）" class="headerlink" title="内存相关（yarn-site.xml）"></a>内存相关（yarn-site.xml）</h2><pre><code class="xml">   &lt;property&gt;  
        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
        &lt;value&gt;7168&lt;/value&gt;
        &lt;description&gt;当前节点可分配的总内存&lt;/description&gt;
   &lt;/property&gt;
   &lt;property&gt; 
        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;
        &lt;value&gt;1024&lt;/value&gt;
        &lt;description&gt;最小可分配容器内存&lt;/description&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;
        &lt;value&gt;7168&lt;/value&gt;
        &lt;description&gt;最大可分配容器内存&lt;/description&gt;
    &lt;/property&gt;
</code></pre>
<h2 id="虚拟内存相关（yarn-site-xml）"><a href="#虚拟内存相关（yarn-site-xml）" class="headerlink" title="虚拟内存相关（yarn-site.xml）"></a>虚拟内存相关（yarn-site.xml）</h2><p>关闭虚拟内存的检查，在执行任务的时候如果分配虚拟内存超出了虚拟内存可用值则会杀掉容器。</p>
<pre><code class="xml">&lt;property&gt;
        &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;
        &lt;value&gt;false&lt;/value&gt;
        &lt;description&gt;默认为true，当前关闭虚拟内存检查&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
        &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;
        &lt;value&gt;2.1&lt;/value&gt;
        &lt;description&gt;如果不关闭虚拟内存检查，并且任务运行需要占用大量内存时可以将改比率调高&lt;/description&gt;
&lt;/property&gt;
</code></pre>
<h2 id="cpu相关（yarn-site-xml）"><a href="#cpu相关（yarn-site-xml）" class="headerlink" title="cpu相关（yarn-site.xml）"></a>cpu相关（yarn-site.xml）</h2><p>cpu和基础设置跟内存一样，4H8G的服务器。预留一个CPU给操作系统以及任务调度运行。其他全部指定给yarn来使用。</p>
<pre><code class="xml">    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;
        &lt;value&gt;7&lt;/value&gt;
        &lt;description&gt;当前节点可分配的总CPU&lt;/description&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.scheduler.minimum-allocation-vcores&lt;/name&gt;
        &lt;value&gt;1&lt;/value&gt;
        &lt;description&gt;最小可分配容器CPU&lt;/description&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt;
        &lt;value&gt;7&lt;/value&gt;
        &lt;description&gt;最大可分配容器CPU&lt;/description&gt;
    &lt;/property&gt;
</code></pre>
<h2 id="其他资源相关配置（yarn-site-xml）"><a href="#其他资源相关配置（yarn-site-xml）" class="headerlink" title="其他资源相关配置（yarn-site.xml）"></a>其他资源相关配置（yarn-site.xml）</h2><p>下面配置用来设置调度器使用的资源比较计算器，默认资源计算器使用：org.apache.hadoop.yarn.util.resource.DefaultResourseCalculator，该资源计算器只根据内存来计算资源，在yarn的UI界面中也不会显示cpu分配的内容，采用DominantResourceCalculator可以多维度计算资源。</p>
<pre><code class="xml">&lt;property&gt;
        &lt;name&gt;yarn.scheduler.capacity.resource-calculator&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.yarn.util.resource.DominantResourceCalculator&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<h2 id="调度器配置（capacity-scheduler-xml）"><a href="#调度器配置（capacity-scheduler-xml）" class="headerlink" title="调度器配置（capacity-scheduler.xml）"></a>调度器配置（capacity-scheduler.xml）</h2><p>该配置项较多，至列特殊几个配置供参考，之前碰到一个场景是我想在当前集群同时运行两个任务，每个任务占总资源的1/2，但是总是后面一个任务在padding中，申请不到资源。所以重点可以关注下面配置是否合理</p>
<pre><code>&lt;property&gt;xml
    &lt;name&gt;yarn.scheduler.capacity.maximum-am-resource-percent&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
    &lt;description&gt;
      运行任务可占用资源的最大百分比，如果该值不是100%则无法使用集群的全部资源并行执行任务
    &lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.scheduler.capacity.root.default.maximum-capacity&lt;/name&gt;
    &lt;value&gt;100&lt;/value&gt;
    &lt;description&gt;
       给root用户（我当前使用的用户）指定最大队列容量
    &lt;/description&gt;
  &lt;/property&gt;
</code></pre>
<h1 id="spark基础配置"><a href="#spark基础配置" class="headerlink" title="spark基础配置"></a>spark基础配置</h1><p>为了让运行的任务使用所有的资源，先计算所有可用资源有多少，当前yarn可分配的一共是3台服务器每台服务器3H7G内存，总计9H21G。</p>
<ul>
<li>假设分配4个容器执行任务，1个容器为ApplicationMaster用作spark资源调度。（–num-executors=4）</li>
<li>ApplicationMaster占用1H1G，剩余8G20G留给任务执行，每个容器可以使用2H5G。（  –executor-cores=2）</li>
<li>5G内容中需要有7%（默认值）的spark.executor.memoryOverhead，则5G*0.07=358.4MB，可以向上取整359MB，则5G-359MB=4761MB（–executor-memory=4761m）</li>
</ul>
<p>总结下来可以在执行任务时如下设置：</p>
<pre><code class="shell">./bin/spark-submit \
  --master yarn \
  --num-executors 4 \
  --executor-memory 4761m \
  --executor-cores 2 
</code></pre>
]]></content>
      <categories>
        <category>大数据</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>YARN</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark on YARN配置资源动态申请</title>
    <url>/2021/07/Spark-on-YARN%E9%85%8D%E7%BD%AE%E8%B5%84%E6%BA%90%E5%8A%A8%E6%80%81%E7%94%B3%E8%AF%B7/</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>运行spark任务时通常任务会按照默认配置正常执行任务，这样当集群中还有空余资源时，这部分资源就会白白浪费，本文介绍如果为spark开启动态资源申请，当有空余资源时，spark任务会自动伸缩来达到更高效的资源利用。</p>
<blockquote>
<p>spark版本： 2.4.1            hadoop版本：2.8.5</p>
</blockquote>
<span id="more"></span>
<h1 id="spark相关配置（spark-defaults-conf）"><a href="#spark相关配置（spark-defaults-conf）" class="headerlink" title="spark相关配置（spark-defaults.conf）"></a>spark相关配置（spark-defaults.conf）</h1><h2 id="开启动态资源申请"><a href="#开启动态资源申请" class="headerlink" title="开启动态资源申请"></a>开启动态资源申请</h2><p>首先需要开启spark动态申请配置spark.dynamicAllocation.enabled，他还需要一些子配置需要设置如：minExecutors、maxExecutors等，大多数情况下为应用配置合适的执行器都需要设置这些子属性。设置这些属性的时候需要大量的试验来确保应用分配正确的执行器个数，过多的分配只会造成资源浪费。下面给出示例配置：</p>
<pre><code>spark.dynamicAllocation.enabled true
spark.dynamicAllocation.minExecutors 1
spark.dynamicAllocation.maxExecutors 10
spark.dynamicAllocation.schedulerBacklogTimeout 1s
spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 5s
</code></pre>
<p>上述配置中额外设置了schedulerBacklogTimeout和sustainedSchedulerBacklogTimeout</p>
<ul>
<li>schedulerBacklogTimeout  表示：如果有处于Pending中的Task等待了一段时间(默认1秒)，则增加executor</li>
<li>sustainedSchedulerBacklogTimeout 表示：在随后每隔N秒(默认1秒)，再检测Pending Task，如果仍然存在，则继续增加executor。</li>
</ul>
<h2 id="开启shuffle服务器"><a href="#开启shuffle服务器" class="headerlink" title="开启shuffle服务器"></a>开启shuffle服务器</h2><p>开启了spark.dynamicAllocation.enabled后还需要启动一个外部shuffle服务，供YARN使用。</p>
<pre><code>spark.shuffle.service.enabled true
spark.shuffle.service.port 7337
</code></pre>
<h1 id="yarn相关配置（yarn-siete-xml）"><a href="#yarn相关配置（yarn-siete-xml）" class="headerlink" title="yarn相关配置（yarn-siete.xml）"></a>yarn相关配置（yarn-siete.xml）</h1><h2 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h2><pre><code class="xml">&lt;property&gt;
　　&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
　　&lt;value&gt;mapreduce_shuffle,spark_shuffle&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
　　&lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt;
　　&lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
　　&lt;name&gt;spark.shuffle.service.port&lt;/name&gt;
　　&lt;value&gt;7337&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<h2 id="额外操作"><a href="#额外操作" class="headerlink" title="额外操作"></a>额外操作</h2><p>为了让yarn支持spark的动态资源申请，还需要添加spark-yarn-shuffle的jar包：spark-<version>-yarn-shuffle.jar，本文使用spark-2.4.1版本，对应jar包位置在SPARK_HOME/yarn/spark-2.4.1-yarn-shuffle.jar。将该jar包放置HADOOP_HOME/share/hadoop/yarn/lib/目录下。</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>YARN</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenResty反向代理增加DNS解析支持</title>
    <url>/2021/08/OpenResty%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E5%A2%9E%E5%8A%A0DNS%E8%A7%A3%E6%9E%90%E6%94%AF%E6%8C%81/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>当前工作中使用了开源的联邦学习框架FATE，其中“on spark”架构中使用到了OpenResty来做网关，docker部署服务时服务间的通讯直接使用docker-hostname来访问的，但是OpenResty中调用set_current_peer()无法直接请求hostname。代码片段如下</p>
<span id="more"></span>
<pre><code>local ngx = ngx
local ngx_balancer = require &quot;ngx.balancer&quot;

local function balance()
    local dest_cluster = ngx.ctx.dest_cluster
    -- 当前代理服务请求目标服务时只支持ip地址请求，dest_cluster实际内容为：ip+端口号
    local ok, err = ngx_balancer.set_current_peer(dest_cluster)
    if not ok then
        return ngx.ERROR
    end
end

balance()
</code></pre>
<h1 id="简介Nginx的resolver"><a href="#简介Nginx的resolver" class="headerlink" title="简介Nginx的resolver"></a>简介Nginx的resolver</h1><p>Nginx本身提供了resolver指令来解析hostname。</p>
<pre><code>Syntax:    resolver address ... [valid=time] [ipv6=on|off];
Default:     —
Context:    http, server, location
</code></pre>
<p>示例：</p>
<pre><code>resolver 8.8.8.8 114.114.114.114 valid=3600s;
</code></pre>
<h1 id="Lua-DNS解析支持"><a href="#Lua-DNS解析支持" class="headerlink" title="Lua-DNS解析支持"></a>Lua-DNS解析支持</h1><p>代码如下，读取/etc/resolv.conf文件中DNS地址，缓存该地址提高运行效率，通过lua-resty-dns库提供的方法解析域名获取实际IP地址。（代码中require “resolver”是用的lua-resty-dns库自定义的名称）</p>
<blockquote>
<p>获得lua-resty-dns请从git下载：<a href="https://github.com/openresty/lua-resty-dns">https://github.com/openresty/lua-resty-dns</a></p>
</blockquote>
<p>代码文件：host_resolver.lua</p>
<pre><code>local _M = &#123;
  _VERSION = &#39;0.1&#39;
&#125;

local resolver = require &quot;resolver&quot;
local pcall = pcall
local io_open = io.open
local ngx_re_gmatch = ngx.re.gmatch
local require = require
local ngx_re_find = ngx.re.find
local lrucache = require &quot;resty.lrucache&quot;
local cache_storage = lrucache.new(200)
local ok, new_tab = pcall(require, &quot;table.new&quot;)

if not ok then
    new_tab = function (narr, nrec) return &#123;&#125; end
end

local _dns_servers = new_tab(5, 0)

local _read_file_data = function(path)
    local f, err = io_open(path, &#39;r&#39;)

    if not f or err then
        return nil, err
    end

    local data = f:read(&#39;*all&#39;)
    f:close()
    return data, nil
end

local _read_dns_servers_from_resolv_file = function()
    local text = _read_file_data(&#39;/etc/resolv.conf&#39;)

    local captures, it, err
    it, err = ngx_re_gmatch(text, [[^nameserver\s+(\d+?\.\d+?\.\d+?\.\d+$)]], &quot;jomi&quot;)

    for captures, err in it do
        if not err then
            _dns_servers[#_dns_servers + 1] = captures[1]
        end
    end
end

local _is_addr = function(hostname)
    return ngx_re_find(hostname, [[\d+?\.\d+?\.\d+?\.\d+$]], &quot;jo&quot;)
end

function _M.resolve(hostname)
  if _is_addr(hostname) then
      return hostname, hostname
  end

  local addr = cache_storage:get(hostname)

  if addr then
      return addr, hostname
  end

  local r, err = resolver:new(&#123;
      nameservers = _dns_servers,
      retrans = 5,  -- 5 retransmissions on receive timeout
      timeout = 2000,  -- 2 sec
  &#125;)

  if not r then
      return nil, hostname
  end

  local answers, err = r:query(hostname, &#123;qtype = r.TYPE_A&#125;)

  if not answers or answers.errcode then
      return nil, hostname
  end

  for i, ans in ipairs(answers) do
      if ans.address then
          cache_storage:set(hostname, ans.address, 300)
          return ans.address, hostname
      end
  end

  return nil, hostname
end

_read_dns_servers_from_resolv_file()

return _M
</code></pre>
<h1 id="解析host"><a href="#解析host" class="headerlink" title="解析host"></a>解析host</h1><p>这样在读取到hostname的时候可以将其转换成ip地址发起调用。</p>
<pre><code>-- 读取上一步编写的配置文件
local host_resolver = require &quot;host_resolver&quot;
-- 解析服务
local server_ip = host_resolver.resolve(&quot;www.baidu.com&quot;)
</code></pre>
<blockquote>
<p>上述代码结合需要结合实际业务放在合适的位置，本文开头调用服务代码没有修改，是在它之前对hostname做了处理保证发起调用是使用ip调用。</p>
</blockquote>
<h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p><a href="https://moonbingbing.gitbooks.io/openresty-best-practices/content/dns/use_dynamic_dns.html">https://moonbingbing.gitbooks.io/openresty-best-practices/content/dns/use_dynamic_dns.html</a><br><a href="https://moonbingbing.gitbooks.io/openresty-best-practices/content/ngx_lua/resolve_the_domain_name.html">https://moonbingbing.gitbooks.io/openresty-best-practices/content/ngx_lua/resolve_the_domain_name.html</a></p>
]]></content>
      <categories>
        <category>运维</category>
        <category>NGXIN</category>
      </categories>
      <tags>
        <tag>OpenResty</tag>
        <tag>NGINX</tag>
        <tag>反向代理</tag>
        <tag>Lua</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop异常问题整理</title>
    <url>/2021/08/Hadoop%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h1 id="磁盘空间未满，提示空间不足"><a href="#磁盘空间未满，提示空间不足" class="headerlink" title="磁盘空间未满，提示空间不足"></a>磁盘空间未满，提示空间不足</h1><h2 id="背景："><a href="#背景：" class="headerlink" title="背景："></a>背景：</h2><p>通过python传输一个16G的文件，通过读取每次读取8MB行信息追加到hadoop中。查看磁盘并没有占满：</p>
<pre><code>$ df -h
Filesystem             Size  Used Avail Use% Mounted on
devtmpfs               7.8G     0  7.8G   0% /dev
tmpfs                  7.8G     0  7.8G   0% /dev/shm
tmpfs                  7.8G  369M  7.4G   5% /run
tmpfs                  7.8G     0  7.8G   0% /sys/fs/cgroup
/dev/vda1               40G   14G   24G  36% /
/dev/mapper/data-data  500G  107G  394G  22% /data
</code></pre>
<p>DataNode日志信息如下：</p>
<pre><code>org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Insufficient space for appending to FinalizedReplica, blk_1073743960_7306, FINALIZED
  getNumBytes()     = 39857529
  getBytesOnDisk()  = 39857529
  getVisibleLength()= 39857529
  getVolume()       = /opt/hadoop/data/dfs/data/current
  getBlockFile()    = /opt/hadoop/data/dfs/data/current/BP-763661453-172.20.0.4-1627020836502/current/finalized/subdir0/subdir8/blk_1073743960
    at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.append(FsDatasetImpl.java:1215)
    at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.append(FsDatasetImpl.java:1177)
    at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.&lt;init&gt;(BlockReceiver.java:220)
    at org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBlockReceiver(DataXceiver.java:1254)
    at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:720)
    at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:166)
    at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:103)
    at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:288)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>
<blockquote>
<p>当前问题是hadoop一个还未处理的BUG：<a href="https://issues.apache.org/jira/browse/HDFS-6489">https://issues.apache.org/jira/browse/HDFS-6489</a></p>
</blockquote>
<h2 id="解决方案："><a href="#解决方案：" class="headerlink" title="解决方案："></a>解决方案：</h2><p>目前在应用层增大每次追加数据量，减少追加次数来尽量避免该问题</p>
<h1 id="hadoop节点间传输数据异常"><a href="#hadoop节点间传输数据异常" class="headerlink" title="hadoop节点间传输数据异常"></a>hadoop节点间传输数据异常</h1><h2 id="背景：-1"><a href="#背景：-1" class="headerlink" title="背景："></a>背景：</h2><p>docker搭建hadoop集群，master和slave节点分别在不同的机器的docker容器中。启动服务上传数据后各个datenode会报警告，异常日志如下：</p>
<pre><code>2021-08-11 10:19:05,628 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(10.35.2.118:50010, datanodeUuid=1f82aee1-caaf-4e61-a6f9-9ba75bd44b8a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-ae77e4c7-fcd2-41ce-804d-ab8acb6120c7;nsid=455830163;c=1627886917835):Failed to transfer BP-372102307-172.18.0.4-1627886917835:blk_1073752046_13292 to 172.18.0.4:50010 got
java.net.NoRouteToHostException: No route to host
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
        at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2320)
        at java.lang.Thread.run(Thread.java:748)
</code></pre>
<h2 id="原因："><a href="#原因：" class="headerlink" title="原因："></a>原因：</h2><p>hadoop默认dfs.datanode.address为0.0.0.0:50010，0.0.0.0对应容器地址已上述日志例子中是172.18.0.4，其他节点访问172.18.0.4是无法访问的，因为是其他机器的虚拟IP地址。</p>
<h2 id="解决方案：-1"><a href="#解决方案：-1" class="headerlink" title="解决方案："></a>解决方案：</h2><p>将dfs.datanode.use.datanode.hostname设置为true（默认是false）。</p>
<pre><code>&lt;property&gt;
        &lt;name&gt;dfs.datanode.use.datanode.hostname&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<p>官方解释如下：</p>
<pre><code>Whether datanodes should use datanode hostnames when connecting to other datanodes for data transfer.
</code></pre>
<p>大致意思为：datanodes与其他节点传输数据时是否使用datanode主机名</p>
]]></content>
      <categories>
        <category>问题总结</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>python使用pyarrow打开haddop大文件异常</title>
    <url>/2021/08/python%E4%BD%BF%E7%94%A8pyarrow%E6%89%93%E5%BC%80haddop%E5%A4%A7%E6%96%87%E4%BB%B6%E5%BC%82%E5%B8%B8/</url>
    <content><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>项目中计算文件行数，使用Pyarror库 HadoopFileSystem.open_input_stream()打开hadoop大文件会被Killed，并且通过spark运行无任何异常日志。</p>
<blockquote>
<p>机器内存：16G   hadoop文件：24G</p>
</blockquote>
<span id="more"></span>
<h1 id="原始代码"><a href="#原始代码" class="headerlink" title="原始代码"></a>原始代码</h1><pre><code>import io
import subprocess
from pyarrow import fs

path = &quot;hdfs://hadoop-master:9000//fate/input_data/epsilon/epsilon_host_500w&quot;
try:
    from pyarrow import HadoopFileSystem
    HadoopFileSystem(path)
except Exception as e:
    print(&quot;error&quot;)
_hdfs_client = fs.HadoopFileSystem.from_uri(path)

def _as_generator():
    with io.TextIOWrapper(buffer=_hdfs_client.open_input_stream(path,buffer_size=1024*1024*8),encoding=&quot;utf-8&quot;) as reader:
        print(&quot;-----准备流结束------&quot;)
        for line in reader:
            yield line


def count():
    count = 0
    for _ in _as_generator():
        count += 1
    print(count)
    return count

if __name__ == &#39;__main__&#39;:
   print(count())
</code></pre>
<p>#监控内存信息：</p>
<pre><code>              total        used        free      shared  buff/cache   available
Mem:            15G        5.3G        9.4G        353M        768M        9.6G
Swap:            0B          0B          0B

              total        used        free      shared  buff/cache   available
Mem:            15G        9.5G        922M        353M        5.1G        5.4G
Swap:            0B          0B          0B

              total        used        free      shared  buff/cache   available
Mem:            15G         12G        154M        353M        2.5G        2.0G
Swap:            0B          0B          0B

              total        used        free      shared  buff/cache   available
Mem:            15G         14G        156M        353M        847M        354M
Swap:            0B          0B          0B
</code></pre>
<h1 id="运行日志-（spark-submit）"><a href="#运行日志-（spark-submit）" class="headerlink" title="运行日志 （spark-submit）"></a>运行日志 （spark-submit）</h1><p>运行在spark中无异常信息日志如下：</p>
<pre><code>21/08/13 03:10:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/08/13 03:10:27 WARN hdfs.DFSClient: zero
-----准备流结束------
21/08/13 11:10:54 INFO util.ShutdownHookManager: Shutdown hook called
21/08/13 11:10:54 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-9ba6ca35-4068-47bb-b09b-5e94cad85d09
</code></pre>
<h1 id="直接用python运行日志如下："><a href="#直接用python运行日志如下：" class="headerlink" title="直接用python运行日志如下："></a>直接用python运行日志如下：</h1><pre><code>21/08/13 03:09:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/08/13 03:09:37 WARN hdfs.DFSClient: zero
-----准备流结束------
Killed
</code></pre>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>直接用python直接调用时会出现Killed的系统异常信息（该信息无法在python层面被捕获）。运行在spark环境中时不会有系统的kill信息显示，生产环境中出现问题后并不好排查。目前推测HadoopFileSystem.open_input_stream()打开的文件流是整个文件大小。</p>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>目前使用hadoop命令来替换之前的流处理，通过子进程执行hadoop fs -cat ${path}返回的标准输出来逐行读取内容。</p>
<pre><code>def _as_generator():
    content_pipe = subprocess.Popen([&quot;hadoop&quot;,&quot;fs&quot;,&quot;-cat&quot;,path], stdout=subprocess.PIPE)
    for line in content_pipe.stdout:
        yield line
</code></pre>
]]></content>
      <categories>
        <category>问题总结</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Python</tag>
        <tag>Pyarrow</tag>
      </tags>
  </entry>
  <entry>
    <title>FATE on RabbitMQ源码分析</title>
    <url>/2021/09/FATE-on-RabbitMQ%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h1 id="接收消息处理详解（GET方法）"><a href="#接收消息处理详解（GET方法）" class="headerlink" title="接收消息处理详解（GET方法）"></a>接收消息处理详解（GET方法）</h1><p>基本流程：<br>读取数据类型——&gt;读取数据信息——-&gt;根据数据类型解析数据——-&gt;返回数据</p>
<h2 id="读取数据类型"><a href="#读取数据类型" class="headerlink" title="读取数据类型"></a>读取数据类型</h2><h3 id="根据入参生成-name-dtype-keys"><a href="#根据入参生成-name-dtype-keys" class="headerlink" title="根据入参生成_name_dtype_keys"></a>根据入参生成_name_dtype_keys</h3><p>name_dtype_key格式为：{role}^{party_id}^{name}^{tag}^get<br>读取【host】方【hash.1920600c69193548404f.host_forward_dict】的【fit.0.0】数据类型示例内容：</p>
<pre><code>role=host
party_id=10000
name=hash.1920600c69193548404f.host_forward_dict
tag=fit.0.0
组成：host^10000^hash.1920600c69193548404f.host_forward_dict^fit.0.0^get
</code></pre>
<p>name_dtype_key会存放在_name_dtype_map中映射对应的传输数据类型</p>
<h3 id="构建数据类型的队列名称"><a href="#构建数据类型的队列名称" class="headerlink" title="构建数据类型的队列名称"></a>构建数据类型的队列名称</h3><p>入参的parties,dtype示例：</p>
<pre><code>parties=[Party(role=host, party_id=10000)])
dtype=&lt;dtype&gt;     【注意该值就是一个写死的字符串做一个标识】
构建队列为：
sed-2021082516
252002585861_hetero_lr_0_0-guest-9999-host-10000-&lt;dtype&gt;-&lt;dtype&gt;
receive-2021082516
252002585861_hetero_lr_0_0-host-10000-guest-9999-&lt;dtype&gt;-&lt;dtype&gt;
</code></pre>
<h3 id="读取mq消息"><a href="#读取mq消息" class="headerlink" title="读取mq消息"></a>读取mq消息</h3><p>核心代码，构建队列名称，获取mq的channel，进行队列消费</p>
<pre><code>mq_names = self._get_mq_names(parties, dtype=NAME_DTYPE_TAG)                       
channel_infos = self._get_channels(mq_names=mq_names)
obj = self._receive_obj(info, name, tag=_SPLIT_.join([tag, NAME_DTYPE_TAG]))
</code></pre>
<h3 id="存储-name-dtype-keys和实际数据类型映射关系"><a href="#存储-name-dtype-keys和实际数据类型映射关系" class="headerlink" title="存储_name_dtype_keys和实际数据类型映射关系"></a>存储_name_dtype_keys和实际数据类型映射关系</h3><p>核心代码，如果_name_dtype_map中没有对应数据类型，则对获取到的数据类型进行存储</p>
<pre><code>for k in _name_dtype_keys:
     if k not in self._name_dtype_map:
         self._name_dtype_map[k] = rtn_dtype[0]
</code></pre>
<h3 id="补充：可传递数据类型"><a href="#补充：可传递数据类型" class="headerlink" title="补充：可传递数据类型"></a>补充：可传递数据类型</h3><p>核心代码如下，可以传输Table类型和Object类型。Table类型后续会用于spark解析，Object类型</p>
<pre><code>class FederationDataType(object):
    OBJECT = &#39;obj&#39;
    TABLE = &#39;Table&#39;
</code></pre>
<h2 id="根据数据类型处理数据"><a href="#根据数据类型处理数据" class="headerlink" title="根据数据类型处理数据"></a>根据数据类型处理数据</h2><h3 id="TABLE类型数据"><a href="#TABLE类型数据" class="headerlink" title="TABLE类型数据"></a>TABLE类型数据</h3><h4 id="构建实际数据的队列名称"><a href="#构建实际数据的队列名称" class="headerlink" title="构建实际数据的队列名称"></a>构建实际数据的队列名称</h4><p>入参的parties,name,partitions示例：</p>
<pre><code>parties=[Party(role=host, party_id=10000)])
name=hash.1920600c69193548404f.host_forward_dict
partitions=1    【用于表示数据分区数】
构建队列为：
send-
2021082516252002585861_hetero_lr_0_0-guest-9999-host-10000-hash.1920600c69193548404f.host_forward_dict-0
receive-
2021082516252002585861_hetero_lr_0_0-host-10000-guest-9999-hash.1920600c69193548404f.host_forward_dict-0
</code></pre>
<h4 id="遍历队列读取数据"><a href="#遍历队列读取数据" class="headerlink" title="遍历队列读取数据"></a>遍历队列读取数据</h4><p>关键代码，构建读取数据的一个回调函数，传输spark来分区读取数据（TABLE类型的特殊处理）<br>mapPartitionsWithIndex方法通过对RDD的每个分区应用函数返回一个新的RDD，同时跟踪原始分区的索引。<br>parallelize构建原始分区索引的RDD对象</p>
<pre><code>receive_func = self._get_partition_receive_func(name, tag, party_id, role, party_mq_names, mq=self._mq, connection_conf=self._rabbit_manager.runtime_config.get(&#39;connection&#39;, &#123;&#125;))
sc = SparkContext.getOrCreate()               
rdd = sc.parallelize(range(partitions), partitions)
rdd = rdd.mapPartitionsWithIndex(receive_func)
</code></pre>
<h3 id="Object类型数据"><a href="#Object类型数据" class="headerlink" title="Object类型数据"></a>Object类型数据</h3><h4 id="构建实际数据的队列名称-1"><a href="#构建实际数据的队列名称-1" class="headerlink" title="构建实际数据的队列名称"></a>构建实际数据的队列名称</h4><p>跟TABLE类型一致，只是没有了partitions参数<br>入参的parties,name示例：</p>
<pre><code>parties=[Party(role=host, party_id=10000)])
name=hash.1920600c69193548404f.host_forward_dict
构建队列为：
send-
2021082516252002585861_hetero_lr_0_0-guest-9999-host-10000-hash.1920600c69193548404f.host_forward_dict
receive-
2021082516252002585861_hetero_lr_0_0-host-10000-guest-9999-hash.1920600c69193548404f.host_forward_dict
</code></pre>
<h4 id="队列读取数据"><a href="#队列读取数据" class="headerlink" title="队列读取数据"></a>队列读取数据</h4><p>核心代码为:</p>
<pre><code>obj = self._receive_obj(info, name, tag)
</code></pre>
<p>_receive_obj方法对数据进行了一些优化，会根据一个cache_key来判断当前数据之前读取过没有，如果已经读取过则返回缓存数据</p>
<pre><code>if wish_cache_key in self._message_cache:
     return self._message_cache[wish_cache_key]
</code></pre>
<p>读取的数据使用pickle.load反序列化</p>
<pre><code>self._message_cache[cache_key] = p_loads(body)
</code></pre>
<h1 id="队列构建"><a href="#队列构建" class="headerlink" title="队列构建"></a>队列构建</h1><p>只要进行数据传输，就需要创建队列，规则如下：</p>
<ul>
<li>队列在每一方，发送或者接收都会有一个send-queue和一个receive-queue</li>
<li>每个本地的receive-queue都会和相应的远程的sedn-queue进行绑定</li>
<li>任何资源在没有的情况下都会主动创建出来（vhost\queue\user）</li>
<li>每一个send/receive对应一个_QueueNames()</li>
<li>每个_QueueNames对应一个queue_key来标识这个队列信息是否已经创建</li>
</ul>
<h2 id="队列的key构建核心代码"><a href="#队列的key构建核心代码" class="headerlink" title="队列的key构建核心代码"></a>队列的key构建核心代码</h2><p>示例：host^10000^hash.1920600c69193548404f.host_forward_dict^0</p>
<pre><code>_SPLIT_ = ^
- 带partitions : queue_key = _SPLIT_.join([party.role, party.party_id, name, str(i)])
- 带名称的：queue_key = _SPLIT_.join([party.role, party.party_id, name])
- 不带名称的：queue_key = _SPLIT_.join([party.role, party.party_id])
</code></pre>
<h2 id="队列构建的结构如下代码所示："><a href="#队列构建的结构如下代码所示：" class="headerlink" title="队列构建的结构如下代码所示："></a>队列构建的结构如下代码所示：</h2><p>self._session_id格式：2021082516252002585861_hetero_lr_0_0<br>self._party.role格式： guest<br>self._party.party_id格式：9999<br>party.role格式：host<br>party.party_id格式：10000<br>queue_suffix格式：hash.1920600c69193548404f.host_forward_dict-0</p>
<pre><code>send_queue_name = f&quot;send-&#123;self._session_id&#125;-&#123;self._party.role&#125;-&#123;self._party.party_id&#125;-&#123;party.role&#125;-&#123;party.party_id&#125;-&#123;queue_suffix&#125;&quot;
 receive_queue_name = f&quot;receive-&#123;self._session_id&#125;-&#123;party.role&#125;-&#123;party.party_id&#125;-&#123;self._party.role&#125;-&#123;self._party.party_id&#125;-&#123;queue_suffix&#125;&quot;  
</code></pre>
<h1 id="流程简述"><a href="#流程简述" class="headerlink" title="流程简述"></a>流程简述</h1><h2 id="接收信息"><a href="#接收信息" class="headerlink" title="接收信息"></a>接收信息</h2><h3 id="1-生成queue-key"><a href="#1-生成queue-key" class="headerlink" title="1.生成queue_key"></a>1.生成queue_key</h3><p>下面是一个传递类型的queue_key</p>
<pre><code>guest^9999^&lt;dtype&gt;^&lt;dtype&gt;
</code></pre>
<h3 id="2-生成发送和接收的queue"><a href="#2-生成发送和接收的queue" class="headerlink" title="2. 生成发送和接收的queue"></a>2. 生成发送和接收的queue</h3><pre><code>vhost：session_id-guest-9999-host-10000  
send：send-guest-9999-guest-9999-&lt;dtype&gt;-&lt;dtype&gt;  
receive: receive-guest-9999-guest-9999-&lt;dtype&gt;-&lt;dtype&gt; 
</code></pre>
<h3 id="3-将queue-key与生成的发送接收队列形成map映射-queue-map"><a href="#3-将queue-key与生成的发送接收队列形成map映射-queue-map" class="headerlink" title="3. 将queue_key与生成的发送接收队列形成map映射_queue_map"></a>3. 将queue_key与生成的发送接收队列形成map映射_queue_map</h3><pre><code>&#123;
        &quot;guest^9999^&lt;dtype&gt;^&lt;dtype&gt;&quot; : &#123; sendQueue, receiveQueue &#125;,
        &quot;host^199999^&lt;dtype&gt;^&lt;dtype&gt;&quot; : &#123; sendQueue, receiveQueue &#125;
&#125;
</code></pre>
<h3 id="4-根据-queue-map相关内容生成channel（MQChannel）"><a href="#4-根据-queue-map相关内容生成channel（MQChannel）" class="headerlink" title="4. 根据_queue_map相关内容生成channel（MQChannel）"></a>4. 根据_queue_map相关内容生成channel（MQChannel）</h3><p>_channels_map映射queue_key和channel关系<br>MQchannel用于生成和消费消息</p>
<h2 id="5-接收所有queue-key的消息"><a href="#5-接收所有queue-key的消息" class="headerlink" title="5. 接收所有queue_key的消息"></a>5. 接收所有queue_key的消息</h2><ul>
<li>Object类型使用pickle反序列化</li>
<li>Table类型使用spark初始化<br>遍历所有channel调用consume() 阻塞消费消息 （这里非异步读取）</li>
</ul>
<h3 id="6-根据上面接收到的数类型进行后续的数据解析"><a href="#6-根据上面接收到的数类型进行后续的数据解析" class="headerlink" title="6. 根据上面接收到的数类型进行后续的数据解析"></a>6. 根据上面接收到的数类型进行后续的数据解析</h3><p>根据参与方信息读取所有的需要接收数据队列，遍历读取数据</p>
<h2 id="发送信息"><a href="#发送信息" class="headerlink" title="发送信息"></a>发送信息</h2><ol>
<li>先发送一条类型信息（单独的queue）</li>
<li>发送具体数据</li>
</ol>
<h1 id="第一次看代码的问题"><a href="#第一次看代码的问题" class="headerlink" title="第一次看代码的问题"></a>第一次看代码的问题</h1><ol>
<li>当前版本并没有调用cleanup的处理 清除mq相关数据（代码中其实有处理，后续进行解读）</li>
<li>mq的每个vhost的user是怎么设置进来的需要读源码解决（在创建任务解析配置文件的时候就会随机定义好，后续读取runtimeConf配置可以拿到）</li>
<li>创建队列的时候只会把对方接受数据的queue名称变成send-xxx-xxxx，不管是发送还是接受都会创建所有的queue，然后同步给远程的只有接受队列（队列发送接收都会被创建，本地的接收队列会和远程发送队列绑定）</li>
</ol>
<h1 id="附录：mq创建upstream示例"><a href="#附录：mq创建upstream示例" class="headerlink" title="附录：mq创建upstream示例"></a>附录：mq创建upstream示例</h1><p>开启federation组件（GUEST、HOST都需开启）</p>
<pre><code>rabbitmq-plugins enable rabbitmq_federation_management
rabbitmq-plugins enable rabbitmq_federation
</code></pre>
<p>GUEST方创建receive_queue_name名称的upstream</p>
<pre><code>rabbitmqctl set_parameter federation-upstream receive-10000-9999 \
&#39;&#123;&quot;uri&quot;:&quot;amqp://guest:guest@172.18.0.3:5672&quot;,&quot;expires&quot;:3600000,&quot;queue&quot;:&quot;send-10000-9999&quot;&#125;&#39;
</code></pre>
<p>GUEST方创建policies</p>
<pre><code>rabbitmqctl set_policy --apply-to queues &quot;receive-10000-9999&quot; &quot;receive-10000-9999&quot; &#39;&#123;&quot;federation-upstream&quot;:&quot;receive-10000-9999&quot;&#125;&#39;
</code></pre>
<p>发送方代码（HOST）：</p>
<pre><code>import pika
import sys
import os


def main():
    # 创建链接
    connection = pika.BlockingConnection(pika.ConnectionParameters(&#39;localhost&#39;, &#39;5673&#39;))
    channel = connection.channel()
    # 确保队列存在
    channel.queue_declare(queue=&#39;send-10000-9999&#39;)

    # 发送消息，所有消息必须经过exchange，存在一个默认的exchange=&#39;&#39;。定义routing_key来确定他传递到哪个队列
    channel.basic_publish(exchange=&#39;&#39;, routing_key=&#39;send-10000-9999&#39;, body=&#39;Hello World!&#39;)
    print(&quot;[x] Sent &#39;Hello World!&#39;&quot;)
    connection.close()


if __name__ == &#39;__main__&#39;:
    try:
        main()
    except KeyboardInterrupt:
        print(&#39;Interrupted&#39;)
        try:
            sys.exit(0)
        except SystemExit:
            os._exit(0)
</code></pre>
<p>接收方代码（GUEST）：</p>
<pre><code>import pika
import os
import sys
import time


def main():
    # 创建链接
    connection = pika.BlockingConnection(pika.ConnectionParameters(host=&#39;localhost&#39;, port=&#39;5672&#39;))
    channel = connection.channel()
    # 确保队列存在
    channel.queue_declare(queue=&#39;receive-10000-9999&#39;)

    # 接收消息，定义callback函数，接收消息回调该函数
    def callback(ch, method, properties, body):
        time.sleep(10)
        print(&quot;[x] Received %r &quot; % body)
        ch.basic_ack(delivery_tag=method.delivery_tag)

    channel.basic_consume(queue=&#39;receive-10000-9999&#39;, auto_ack=False, on_message_callback=callback)
    print(&#39; [*] Waiting for messages. To exit press CTRL+C&#39;)
    channel.start_consuming()


if __name__ == &#39;__main__&#39;:
    try:
        main()
    except KeyboardInterrupt:
        print(&#39;Interrupted&#39;)
        try:
            sys.exit(0)
        except SystemExit:
            os._exit(0)
</code></pre>
<p>运行结果如下：<br><img src="/1.jpg"><br><img src="/2.jpg"></p>
<h1 id="附录：FATE运行时MQ截图示例"><a href="#附录：FATE运行时MQ截图示例" class="headerlink" title="附录：FATE运行时MQ截图示例"></a>附录：FATE运行时MQ截图示例</h1><p><img src="/3.jpg"><br><img src="/4.jpg"><br><img src="/5.jpg"><br><img src="/6.jpg"></p>
]]></content>
      <categories>
        <category>联邦学习</category>
        <category>FATE</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
        <tag>FATE</tag>
      </tags>
  </entry>
  <entry>
    <title>FATE-文件上传流程整理(附：Java序列化实现样例)</title>
    <url>/2021/09/FATE-%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E6%B5%81%E7%A8%8B%E6%95%B4%E7%90%86-%E9%99%84%EF%BC%9AJava%E5%BA%8F%E5%88%97%E5%8C%96%E5%AE%9E%E7%8E%B0%E6%A0%B7%E4%BE%8B/</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>本文梳理微众银行-FATE框架On Spark架构下，文件上传的基本执行流程和关键的数据处理点，从而使用现有业务框架来改造文件上传流程，实现业务系统多数据源的接入。</p>
<blockquote>
<p>FATE在处理数据的序列化时使用了Python常用的Pickle库进行序列化，本文找到了Java的替代方案，后面有代码样例</p>
</blockquote>
<span id="more"></span>

<h1 id="FATE原生HTTP上传文件示例："><a href="#FATE原生HTTP上传文件示例：" class="headerlink" title="FATE原生HTTP上传文件示例："></a>FATE原生HTTP上传文件示例：</h1><pre><code>curl -F &quot;file=@/Users/rain/Downloads/ychy_test_guest.csv&quot;   &quot;10.128.161.238:9380/v1/data/upload?namespace=ychy_test&amp;table_name=test1_guest&amp;work_mode=1&amp;head=1&amp;partition=16&quot;
</code></pre>
<blockquote>
<p>接口路径：/v1/data/upload</p>
</blockquote>
<h1 id="接口处理流程"><a href="#接口处理流程" class="headerlink" title="接口处理流程"></a>接口处理流程</h1><p>fate_flow/apps/data_access_app.py —–&gt; download_upload(“upload”)</p>
<ul>
<li>保存文件至本地临时目录： /job_id/fate_upload_tmp</li>
<li>构建required_arguments[‘work_mode’,’namespace’,’table_name’,’file’,’head’,’partition’]</li>
<li>构建job_config，补充一些额外的配置参数（table_name转成name，backend如果没有设置默认的0，drop=1配置destory=True）</li>
<li>存储TableMeta信息至：t_storage_table_meta表下 （存储table_name和namespace）</li>
<li>构建job_dsl和job_runtime_conf</li>
<li>提交任务—-&gt; DAGScheduler.submit()</li>
</ul>
<h1 id="调度job"><a href="#调度job" class="headerlink" title="调度job"></a>调度job</h1><p>fate_flow/scheduler/dag_scheduler.py—–&gt;run_do() —-&gt;schedule_running_job()——&gt;update_job_on_initiator()<br>fate_flow/scheduler/federated_scheduler.py —–&gt; start_job()——-&gt;job_command(command=’start’)<br>fate_flow/scheduling_apps/party_app.py——-&gt;start_job()<br>fate_flow/comtroller/task_controller.py———&gt;start_job()——&gt;update_job_status()——-&gt;update_job()</p>
<h1 id="调度task"><a href="#调度task" class="headerlink" title="调度task"></a>调度task</h1><p>fate_flow/scheduler/task_scheduler.py —–&gt; schedule()——&gt;start_task()<br>fate_flow/scheduler/federated_scheduler.py —–&gt; start_task()——-&gt;task_command(command=’start’)<br>fate_flow/scheduling_apps/party_app.py——-&gt;start_task()<br>fate_flow/comtroller/task_controller.py———&gt;start_task()——-&gt;构建spark启动命令 执行TaskExecutor</p>
<h1 id="TaskExecutor-component-name-”uplod-0”"><a href="#TaskExecutor-component-name-”uplod-0”" class="headerlink" title="TaskExecutor(component_name=”uplod_0”)"></a>TaskExecutor(component_name=”uplod_0”)</h1><p>fate_flow/operation/task_executor.py——&gt;run_task()</p>
<ul>
<li>解析参数构建：task_info对象、job_dsl对象、job_runtime_conf对象</li>
<li>解析ComponentName=Upload</li>
<li>解析run_class_package=’.’.join(run_class_paths[:-2]) + ‘.’ + run_class_paths[-2].replace(‘.py’, ‘’) (核心为setting_conf_prefix配置的地址读取模型对应处理类路径)</li>
<li>解析run_class_name=run_class_paths[-1]</li>
<li>初始化Session（computing\federation）</li>
<li>构建run_object对象（根据配置解析具体的Python类，根据run_class_package和run_class_name）</li>
<li>run_object.run()执行方法：fate_flow/components/upload.py —–&gt; run()</li>
<li>储存返回的数据—–&gt;run_object.save_data()</li>
<li>备注：储存返回的模型—–&gt;run_object.export_model()</li>
</ul>
<h1 id="Upload-run-具体执行"><a href="#Upload-run-具体执行" class="headerlink" title="Upload.run()具体执行"></a>Upload.run()具体执行</h1><p>fate_flow/components/upload.py</p>
<ul>
<li>设置head</li>
<li>校验partition：partitions &lt;= 0 or partitions &gt;= self.MAX_PARTITIONS</li>
<li>如果drop=1删除旧的数据</li>
<li>构建上传文件地址：t_address</li>
<li>上传文件至hadoop</li>
<li>保存meta信息</li>
<li>移除临时文件</li>
</ul>
<h1 id="上传文件至hadoop"><a href="#上传文件至hadoop" class="headerlink" title="上传文件至hadoop"></a>上传文件至hadoop</h1><ul>
<li>读取文件行数（采用InpuStrem.readline()）</li>
<li>如果head=true，将第一行信息存储在数据库中：f_schema（此处使用序列化）</li>
<li>遍历每次读取部分行进行追加存储至hadoop（块大小：1024<em>1024</em>256）,行保存在一个List&lt;String[]&gt;中，分隔第一个值（ID）和后面的实际数据</li>
<li>部分数据追加至数据库的f_part_of_data字段中（最多存放100行）</li>
</ul>
<h1 id="核心python上传文件代码"><a href="#核心python上传文件代码" class="headerlink" title="核心python上传文件代码"></a>核心python上传文件代码</h1><pre><code>def save_data_table(self, job_id, dst_table_name, dst_table_namespace, head=True):
        input_file = self.parameters[&quot;file&quot;]
        input_feature_count = self.get_count(input_file)
        with open(input_file, &#39;r&#39;) as fin:
            lines_count = 0
            if head is True:
                data_head = fin.readline()
                input_feature_count -= 1
                self.table.get_meta().update_metas(schema=data_utils.get_header_schema(header_line=data_head, id_delimiter=self.parameters[&quot;id_delimiter&quot;]))
            n = 0
            while True:
                data = list()
                lines = fin.readlines(self.MAX_BYTES)
                if lines:
                    for line in lines:
                        values = line.rstrip().split(self.parameters[&quot;id_delimiter&quot;])
                        data.append((values[0], data_utils.list_to_str(values[1:], id_delimiter=self.parameters[&quot;id_delimiter&quot;])))
                    lines_count += len(data)
                    save_progress = lines_count/input_feature_count*100//1
                    job_info = &#123;&#39;progress&#39;: save_progress, &quot;job_id&quot;: job_id, &quot;role&quot;: self.parameters[&quot;local&quot;][&#39;role&#39;], &quot;party_id&quot;: self.parameters[&quot;local&quot;][&#39;party_id&#39;]&#125;
                    ControllerClient.update_job(job_info=job_info)
                    self.table.put_all(data)
                    if n == 0:
                        self.table.get_meta().update_metas(part_of_data=data)
                else:
                    table_count = self.table.count()
                    self.table.get_meta().update_metas(count=table_count, partitions=self.parameters[&quot;partition&quot;])
                    return table_count
                n += 1
</code></pre>
<h1 id="核心hadoop上传代码"><a href="#核心hadoop上传代码" class="headerlink" title="核心hadoop上传代码"></a>核心hadoop上传代码</h1><pre><code>def put_all(self, kv_list: Iterable, append=True, assume_file_exist=False, **kwargs):
        LOGGER.info(f&quot;put in hdfs file: &#123;self._path&#125;&quot;)
        if append and (assume_file_exist or self._exist()):
            stream = self._hdfs_client.open_append_stream(path=self._path, compression=None)
        else:
            stream = self._hdfs_client.open_output_stream(path=self._path, compression=None)
        counter = 0
        with io.TextIOWrapper(stream) as writer:
            for k, v in kv_list:
                writer.write(hdfs_utils.serialize(k, v))
                writer.write(hdfs_utils.NEWLINE)
                counter = counter + 1
        self._meta.update_metas(count=counter)
</code></pre>
<h1 id="处理hadoop追加数据序列化核心代码"><a href="#处理hadoop追加数据序列化核心代码" class="headerlink" title="处理hadoop追加数据序列化核心代码"></a>处理hadoop追加数据序列化核心代码</h1><pre><code class="python">import pickle

_DELIMITER = &#39;\t&#39;
NEWLINE = &#39;\n&#39;
def deserialize(m):
    fields = m.partition(_DELIMITER)
    return fields[0], pickle.loads(bytes.fromhex(fields[2]))

def serialize(k, v):
    return f&quot;&#123;k&#125;&#123;_DELIMITER&#125;&#123;pickle.dumps(v).hex()&#125;&quot;
</code></pre>
<h1 id="处理schema序列化核心代码"><a href="#处理schema序列化核心代码" class="headerlink" title="处理schema序列化核心代码"></a>处理schema序列化核心代码</h1><pre><code>class SerializedField(LongTextField):
    def db_value(self, value):
        return serialize_b64(value, to_str=True)

    def python_value(self, value):
        return deserialize_b64(value
</code></pre>
<h1 id="schema变化流程"><a href="#schema变化流程" class="headerlink" title="schema变化流程"></a>schema变化流程</h1><p>—-原始数据—-：id,y,x0,x1,x2,x3,x4,x5,x6,x7,x8,x9<br>—-处理后数据—-：{‘header’: ‘y,x0,x1,x2,x3,x4,x5,x6,x7,x8,x9’, ‘sid’: ‘id’}<br>—-数据库存储的序列化数据—-：gAN9cQAoWAYAAABoZWFkZXJxAVgfAAAAeSx4MCx4MSx4Mix4Myx4NCx4NSx4Nix4Nyx4OCx4OXECWAMAAABzaWRxA1gCAAAAaWRxBHUu</p>
<h1 id="行数据变化流程"><a href="#行数据变化流程" class="headerlink" title="行数据变化流程"></a>行数据变化流程</h1><p>—-原始数据—-：133,1,0.254879,-1.046633,0.209656,0.074214,-0.441366,-0.377645,-0.485934,0.347072,-0.287570,-0.733474<br>—-数据库反序列化后的数据—-：<br>[(‘133’,’1,0.254879,-1.046633,0.209656,0.074214,-0.441366,-0.377645,-0.485934,0.347072,-0.287570,-0.733474’)]<br>—hadoop存储的数据—-:<br>133\t80035861000000312c302e3235343837392c2d312e3034363633332c302e3230393635362c302e3037343231342c2d302e3434313336362c2d302e3337373634352c2d302e3438353933342c302e3334373037322c2d302e3238373537302c2d302e37333334373471002e</p>
<h1 id="java实现python-pickle序列化-反序列化"><a href="#java实现python-pickle序列化-反序列化" class="headerlink" title="java实现python-pickle序列化/反序列化"></a>java实现python-pickle序列化/反序列化</h1><p>注意：java序列化使用的是2的协议，pickle支持1-4协议来序列化，java可以支持1-4的反序列化。<br>造成的现象就是java序列化存储在数据库的内容跟python存储的不一样的，但是不影响数据读取</p>
<pre><code class="java">&lt;dependency&gt;
            &lt;groupId&gt;net.razorvine&lt;/groupId&gt;
            &lt;artifactId&gt;pickle&lt;/artifactId&gt;
            &lt;version&gt;1.1&lt;/version&gt;
 &lt;/dependency&gt;

package com.ycy.tool;

import net.razorvine.pickle.PickleException;
import net.razorvine.pickle.Pickler;
import org.springframework.util.Assert;

import java.io.IOException;
import java.nio.charset.StandardCharsets;
import java.util.Base64;

public class Utils &#123;
    private static Pickler pickler = new Pickler();
    private static final char[] HEX_CHAR = &#123; &#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39; &#125;;

    public static byte[] base64Encode(byte[] bytes) &#123;
        return Base64.getEncoder().encode(bytes);
    &#125;

    public static String pickleSerializeBase64(Object obj) throws PickleException, IOException &#123;
        byte[] pickleStr = pickler.dumps(obj);
        byte[] byteStr = base64Encode(pickleStr);
        return new String(byteStr, StandardCharsets.UTF_8);
    &#125;

    public static String pickleDumpsHex(Object obj) throws PickleException, IOException &#123;
        byte[] pickleStr = pickler.dumps(obj);
        return toHexString(pickleStr);
    &#125;

    /**
     * byte[]--&gt;hexString
     * @explain 使用位运算
     * @param bytes
     * @return
     */
    public static String toHexString(byte[] bytes) &#123;
        char[] buf = new char[bytes.length * 2];
        int index = 0;
        // 利用位运算进行转换，可以看作方法二的变型
        for (byte b : bytes) &#123;
            buf[index++] = HEX_CHAR[b &gt;&gt;&gt; 4 &amp; 0xf];
            buf[index++] = HEX_CHAR[b &amp; 0xf];
        &#125;
        return new String(buf);
    &#125;

    public static void main(String[] args) throws Exception &#123;
        String pickleSerialize = pickleSerializeBase64(&quot;&#123;&#39;header&#39;: &#39;y,x0,x1,x2,x3,x4,x5,x6,x7,x8,x9&#39;, &#39;sid&#39;: &#39;id&#39;&#125;&quot;);
        System.out.println(pickleSerialize);
        String pickleHexo = pickleDumpsHex(&quot;1,0.254879,-1.046633,0.209656,0.074214,-0.441366,-0.377645,-0.485934,0.347072,-0.287570,-0.733474&quot;);
        System.out.println(pickleHexo);
        Assert.isTrue(pickleSerialize.equals(&quot;gAJYOgAAAHsnaGVhZGVyJzogJ3kseDAseDEseDIseDMseDQseDUseDYseDcseDgseDknLCAnc2lkJzogJ2lkJ31xAC4=&quot;),&quot;结果异常&quot;);
        Assert.isTrue(pickleHexo.equals(&quot;80025861000000312c302e3235343837392c2d312e3034363633332c302e3230393635362c302e3037343231342c2d302e3434313336362c2d302e3337373634352c2d302e3438353933342c302e3334373037322c2d302e3238373537302c2d302e37333334373471002e&quot;),&quot;结果异常&quot;);
    &#125;
&#125;
</code></pre>
<h1 id="存储表的Meta信息"><a href="#存储表的Meta信息" class="headerlink" title="存储表的Meta信息"></a>存储表的Meta信息</h1><pre><code>f_create_date: 2021-08-14 10:47:48
f_update_date: 2021-08-14 10:47:48
f_name: 00ef4e3cfcaa11eba3af0242ac120004
f_namespace: output_data_202108131124133566128_hetero_lr_0_0
f_address: &#123;&quot;name_node&quot;: &quot;hdfs://hadoop-master:9000&quot;, &quot;path&quot;: &quot;/fate/output_data/output_data_202108131124133566128_hetero_lr_0_0/00ef4e3cfcaa11eba3af0242ac120004&quot;&#125;
f_engine: HDFS
f_type: LMDB
f_options: &#123;&#125;
f_partitions: 90
f_id_delimiter: NULL
f_in_serialized: 1
f_have_head: 1
f_schema: gAN9cQAoWAYAAA  # 序列化
f_count: 500000
f_part_of_data: gANdcQA  # 序列化  最多存100行
f_description:
f_create_time:
f_update_time:
</code></pre>
<h1 id="备注：t-address"><a href="#备注：t-address" class="headerlink" title="备注：t_address"></a>备注：t_address</h1><pre><code>&#123;&quot;name_node&quot;: &quot;hdfs://hadoop-master:9000&quot;, &quot;path&quot;: &quot;/fate/output_data/output_data_202108131124133566128_hetero_lr_0_0/00ef4e3cfcaa11eba3af0242ac120004&quot;&#125;
</code></pre>
<h1 id="备注：上传参数"><a href="#备注：上传参数" class="headerlink" title="备注：上传参数"></a>备注：上传参数</h1><pre><code class="json">&#123;
    &#39;file&#39;: &#39;/data/projects/cw-mpc/jobs/202108161113258966603/fate_upload_tmp/test_data_guest_epsilon_20w.csv&#39;,
    &#39;head&#39;: 1,  # 是否有数据头部
    &#39;id_delimiter&#39;: &#39;,&#39;,
    &#39;partition&#39;: 16,
    &#39;namespace&#39;: &#39;epsilon&#39;,
    &#39;name&#39;: &#39;epsilon_guest_20w_test&#39;,
    &#39;storage_engine&#39;: &#39;&#39;,
    &#39;storage_address&#39;: None,
    &#39;destroy&#39;: Tue
&#125;
</code></pre>
<h1 id="备注：解析Component类路径"><a href="#备注：解析Component类路径" class="headerlink" title="备注：解析Component类路径"></a>备注：解析Component类路径</h1><pre><code>setting_conf_path = os.path.join(file_utils.get_python_base_directory(), *[&#39;federatedml&#39;, &#39;conf&#39;,&#39;setting_conf&#39;])
# 构造：python/federatedml/conf/setting_conf
</code></pre>
<p>通过DslParse来解析上述的路径，上传文件的对应内容为：</p>
<pre><code>&#123;
    &quot;module_path&quot;:  &quot;fate_flow/components&quot;,
    &quot;param_class&quot; : &quot;fate_flow/components/param/upload_param.py/UploadParam&quot;,
    &quot;role&quot;:
    &#123;
        &quot;local&quot;:
        &#123;
            &quot;program&quot;: &quot;upload.py/Upload&quot;
        &#125;
    &#125;
&#125;
</code></pre>
]]></content>
      <categories>
        <category>联邦学习</category>
        <category>FATE</category>
      </categories>
      <tags>
        <tag>FATE</tag>
        <tag>联邦学习</tag>
        <tag>隐私计算</tag>
      </tags>
  </entry>
</search>
